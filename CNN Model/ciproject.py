# -*- coding: utf-8 -*-
"""CIProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g3hTmVYisjjW468bD1Ih8ZVdKM7r9Omr
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import classification_report, confusion_matrix
from tensorflow.keras.models import Sequential, save_model, load_model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.optimizers import Adam
import seaborn as sns

# Google Colab setup
from google.colab import drive
drive.mount('/content/drive')

# Define the paths to your Excel files
file_path_source1 = '/content/drive/MyDrive/CI PROJECT/Datasets/Dataset/Object-1/Object-1.xlsx'  # Adjust the path as needed
file_path_source2 = '/content/drive/MyDrive/CI PROJECT/Datasets/Dataset/Object-2/Object-2.xlsx'  # Adjust the path as needed

# Check available sheet names in the files
xls1 = pd.ExcelFile(file_path_source1)
xls2 = pd.ExcelFile(file_path_source2)

# Display the sheet names
print("Sheet names in Source 1 file:", xls1.sheet_names)
print("Sheet names in Source 2 file:", xls2.sheet_names)

# Use the correct sheet names identified from the previous step
correct_sheet_name_source1 = 'record_data_2020-10-29_10-41-1_'  # Replace with the actual sheet name found for Source 1
correct_sheet_name_source2 = 'record_data_2020-10-29_10-30-40'  # Replace with the actual sheet name found for Source 2

# Load datasets from the identified correct sheet names
data_source1 = pd.read_excel(file_path_source1, sheet_name=correct_sheet_name_source1, usecols="G:DZZ")  # Adjust "G:Z" range as per your data
data_source2 = pd.read_excel(file_path_source2, sheet_name=correct_sheet_name_source2, usecols="G:DZZ")  # Adjust "G:Z" range as per your data

# Display the first few rows to verify correct loading
print(data_source1.head())
print(data_source2.head())


# Convert dataframes to NumPy arrays for plotting
signals_source1 = data_source1.to_numpy()
signals_source2 = data_source2.to_numpy()

# Plotting the first signal from source 1
plt.figure(figsize=(15, 6))
plt.plot(signals_source1[0], label='Source 1 Signal Sample 1')  # Adjust indexing to plot specific signals
plt.title('Signal from Source 1')
plt.xlabel('Time')
plt.ylabel('Amplitude')
plt.legend()
plt.grid(True)
plt.show()

# Plotting the first signal from source 2
plt.figure(figsize=(15, 6))
plt.plot(signals_source2[0], label='Source 2 Signal Sample 1')  # Adjust indexing to plot specific signals
plt.title('Signal from Source 2')
plt.xlabel('Time')
plt.ylabel('Amplitude')
plt.legend()
plt.grid(True)
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import classification_report, confusion_matrix
from tensorflow.keras.models import Sequential, save_model, load_model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.optimizers import Adam
import seaborn as sns

# Google Colab setup
from google.colab import drive
drive.mount('/content/drive')

# Define the paths to your Excel files
file_path_source1 = '/content/drive/MyDrive/CI PROJECT/Datasets/Dataset/Object-1/Object-1.xlsx'  # Adjust the path as needed
file_path_source2 = '/content/drive/MyDrive/CI PROJECT/Datasets/Dataset/Object-2/Object-2.xlsx'  # Adjust the path as needed

# Check available sheet names in the files
xls1 = pd.ExcelFile(file_path_source1)
xls2 = pd.ExcelFile(file_path_source2)

# Display the sheet names
print("Sheet names in Source 1 file:", xls1.sheet_names)
print("Sheet names in Source 2 file:", xls2.sheet_names)

# Use the correct sheet names identified from the previous step
correct_sheet_name_source1 = 'record_data_2020-10-29_10-41-1_'  # Replace with the actual sheet name found for Source 1
correct_sheet_name_source2 = 'record_data_2020-10-29_10-30-40'  # Replace with the actual sheet name found for Source 2

# Load datasets from the identified correct sheet names
data_source1 = pd.read_excel(file_path_source1, sheet_name=correct_sheet_name_source1, usecols="G:DZZ")  # Adjust "G:DZZ" range as per your data
data_source2 = pd.read_excel(file_path_source2, sheet_name=correct_sheet_name_source2, usecols="G:DZZ")  # Adjust "G:DZZ" range as per your data

# Display the first few rows to verify correct loading
print(data_source1.head())
print(data_source2.head())

# Convert dataframes to NumPy arrays for plotting
signals_source1 = data_source1.to_numpy()
signals_source2 = data_source2.to_numpy()

# Function to plot all signals from a given source
def plot_signals(signals, source_name):
    for i in range(len(signals)):
        plt.figure(figsize=(15, 6))
        plt.plot(signals[i], label=f'{source_name} Signal Sample {i+1}')
        plt.title(f'Signal from {source_name} - Sample {i+1}')
        plt.xlabel('Time')
        plt.ylabel('Amplitude')
        plt.legend()
        plt.grid(True)
        plt.show()

# Plot all signals from Source 1
print("Plotting all signals from Source 1...")
plot_signals(signals_source1, "Source 1")

# Plot all signals from Source 2
print("Plotting all signals from Source 2...")
plot_signals(signals_source2, "Source 2")

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc
import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import matplotlib.pyplot as plt
import seaborn as sns
import random
import pywt  # Import for wavelet denoising

# Load your data
file_path_source1 = '/content/drive/MyDrive/CI PROJECT/Datasets/Dataset/Object-1/Object-1.xlsx'
file_path_source2 = '/content/drive/MyDrive/CI PROJECT/Datasets/Dataset/Object-2/Object-2.xlsx'

# Load datasets from the correct sheet names
data_source1 = pd.read_excel(file_path_source1, sheet_name='record_data_2020-10-29_10-41-1_', usecols="G:DZZ")
data_source2 = pd.read_excel(file_path_source2, sheet_name='record_data_2020-10-29_10-30-40', usecols="G:DZZ")

# Combine data
data = pd.concat([data_source1, data_source2], axis=0, ignore_index=True)

# Extract features and labels
X = data.iloc[:, 7:].values  # Feature columns
y = np.concatenate([np.zeros(len(data_source1)), np.ones(len(data_source2))])  # Labels

# Check for NaNs and replace them
X = np.nan_to_num(X)  # Replaces NaNs with zero (or consider using other strategies)

# Wavelet denoising
def denoise_signal(signal):
    wavelet = 'db4'  # Daubechies wavelet
    coeffs = pywt.wavedec(signal, wavelet)
    threshold = np.median(np.abs(coeffs[-1])) / 0.6745
    denoised_coeffs = [pywt.threshold(c, threshold, mode='soft') for c in coeffs]
    return pywt.waverec(denoised_coeffs, wavelet)

X_denoised = np.array([denoise_signal(x) for x in X])

# Normalize features
scaler = StandardScaler()
X_denoised = scaler.fit_transform(X_denoised)

# Reshape features for 1D CNN
X_denoised = X_denoised.reshape(X_denoised.shape[0], X_denoised.shape[1], 1)  # Shape: (samples, sequence_length, 1)

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X_denoised, y, test_size=0.3, stratify=y, random_state=42)

# Function to create the model using Conv1D
def create_model(input_shape):
    model = Sequential([
        Input(shape=input_shape),
        Conv1D(8, 3, activation='relu', padding='same'),
        MaxPooling1D(pool_size=2),
        Conv1D(16, 3, activation='relu', padding='same'),
        MaxPooling1D(pool_size=2),
        Conv1D(32, 3, activation='relu', padding='same'),
        MaxPooling1D(pool_size=2),
        Flatten(),
        Dense(64, activation='relu'),
        Dropout(0.5),  # Dropout to prevent overfitting
        Dense(1, activation='sigmoid')  # Binary classification
    ])

    optimizer = Adam(learning_rate=1e-5)  # Small learning rate
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

    return model

# Create and train the model
model = create_model(input_shape=X_train.shape[1:])

# Callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-3)

# Train the model
history = model.fit(
    X_train, y_train,
    epochs=35, batch_size=64,  # Adjust as needed
    validation_data=(X_val, y_val),
    callbacks=[early_stopping, lr_scheduler],
    verbose=1
)

# Save the model
model_path = '/content/drive/MyDrive/CI_PROJECT/pretrained_model.h5'
model.save(model_path)
print(f"Model saved to {model_path}")

# Load the saved model
loaded_model = load_model(model_path)
print("Model loaded successfully!")

# Evaluate the model
val_predictions = (loaded_model.predict(X_val) > 0.5).astype(int)
val_probabilities = loaded_model.predict(X_val).ravel()

accuracy = accuracy_score(y_val, val_predictions)
precision = precision_score(y_val, val_predictions, zero_division=0)
recall = recall_score(y_val, val_predictions)
f1 = f1_score(y_val, val_predictions)
specificity = recall_score(y_val, val_predictions, pos_label=0)
fpr, tpr, _ = roc_curve(y_val, val_probabilities)
roc_auc = auc(fpr, tpr)

conf_matrix = confusion_matrix(y_val, val_predictions)

# Plot training and validation loss and accuracy curves
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('Training and Validation Loss')

plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Training and Validation Accuracy')
plt.show()

# Test on random samples from both sources
sample_indices = random.sample(range(len(X_val)), 5)
unseen_data = X_val[sample_indices]
unseen_labels = y_val[sample_indices]

# Predict using the loaded model
unseen_predictions = (loaded_model.predict(unseen_data) > 0.5).astype(int)
unseen_accuracy = accuracy_score(unseen_labels, unseen_predictions)

# Display predictions on unseen data
plt.figure(figsize=(10, 6))
plt.bar(range(len(unseen_predictions)), unseen_predictions.ravel(), label='Predicted', alpha=0.6)
plt.bar(range(len(unseen_labels)), unseen_labels, label='True', alpha=0.6)
plt.xlabel('Sample Index')
plt.ylabel('Prediction')
plt.title(f'Predictions on Unseen Data (Accuracy: {unseen_accuracy:.2f})')
plt.legend()
plt.show()

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, accuracy_score
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt

# Load your data
file_path_source1 = '/content/drive/MyDrive/CI PROJECT/Datasets/Dataset/Object-1/Object-1.xlsx'  # Adjust the path as needed
file_path_source2 = '/content/drive/MyDrive/CI PROJECT/Datasets/Dataset/Object-2/Object-2.xlsx'  # Adjust the path as needed

# Adjust sheet names as needed
correct_sheet_name_source1 = 'record_data_2020-10-29_10-41-1_'
correct_sheet_name_source2 = 'record_data_2020-10-29_10-30-40'

# Load datasets from the identified correct sheet names
data_source1 = pd.read_excel(file_path_source1, sheet_name=correct_sheet_name_source1, usecols="G:DZZ")
data_source2 = pd.read_excel(file_path_source2, sheet_name=correct_sheet_name_source2, usecols="G:DZZ")

# Combine data
data = pd.concat([data_source1, data_source2], axis=0, ignore_index=True)

# Extract features and labels
X = data.iloc[:, 7:].values  # Feature columns (adjust based on your data)
y = np.concatenate([np.zeros(len(data_source1)), np.ones(len(data_source2))])  # Labels

# Normalize features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Reshape features for CNN
X = X.reshape(X.shape[0], 1, X.shape[1], 1)  # Shape: (samples, height, width, channels)

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import matplotlib.pyplot as plt
import seaborn as sns

# Load your data
file_path_source1 = '/content/drive/MyDrive/CI PROJECT/Datasets/Dataset/Object-1/Object-1.xlsx'
file_path_source2 = '/content/drive/MyDrive/CI PROJECT/Datasets/Dataset/Object-2/Object-2.xlsx'

# Load datasets from the correct sheet names
data_source1 = pd.read_excel(file_path_source1, sheet_name='record_data_2020-10-29_10-41-1_', usecols="G:DZZ")
data_source2 = pd.read_excel(file_path_source2, sheet_name='record_data_2020-10-29_10-30-40', usecols="G:DZZ")

# Combine data
data = pd.concat([data_source1, data_source2], axis=0, ignore_index=True)

# Extract features and labels
X = data.iloc[:, 7:].values  # Feature columns
y = np.concatenate([np.zeros(len(data_source1)), np.ones(len(data_source2))])  # Labels

# Check for NaNs and replace them
X = np.nan_to_num(X)  # Replaces NaNs with zero (or consider using other strategies)

# Normalize features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Reshape features for CNN
X = X.reshape(X.shape[0], 1, X.shape[1], 1)  # Shape: (samples, height, width, channels)

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)

# Function to create the model
def create_model(input_shape):
    model = Sequential([
        Input(shape=input_shape),
        Conv2D(8, (3, 3), activation='relu', padding='same'),
        MaxPooling2D((1, 2)),
        Conv2D(16, (3, 3), activation='relu', padding='same'),
        MaxPooling2D((1, 2)),
        Conv2D(32, (3, 3), activation='relu', padding='same'),
        MaxPooling2D((1, 2)),
        Flatten(),
        Dense(64, activation='relu'),
        Dropout(0.3),  # Dropout to prevent overfitting
        Dense(1, activation='sigmoid')  # Binary classification
    ])

    optimizer = Adam(learning_rate=1e-5)  # Small learning rate
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

    return model

# Create and train the model
model = create_model(input_shape=X_train.shape[1:])

# Callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-4)

# Train the model
history = model.fit(
    X_train, y_train,
    epochs=50, batch_size=32,  # Adjust as needed
    validation_data=(X_val, y_val),
    callbacks=[early_stopping, lr_scheduler],
    verbose=1
)

# Evaluate the model
val_predictions = (model.predict(X_val) > 0.5).astype(int)
val_probabilities = model.predict(X_val).ravel()

accuracy = accuracy_score(y_val, val_predictions)
precision = precision_score(y_val, val_predictions, zero_division=0)
recall = recall_score(y_val, val_predictions)
f1 = f1_score(y_val, val_predictions)
specificity = recall_score(y_val, val_predictions, pos_label=0)
fpr, tpr, _ = roc_curve(y_val, val_probabilities)
roc_auc = auc(fpr, tpr)

conf_matrix = confusion_matrix(y_val, val_predictions)

# Save the model
model_path = '/content/drive/MyDrive/CI_PROJECT/pretrained_model.h5'
model.save(model_path)
print(f"Model saved to {model_path}")

# Display performance metrics graphically
metrics_df = pd.DataFrame([{
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1 Score': f1,
    'Specificity': specificity,
    'AUC': roc_auc
}])
metrics_df.plot(kind='bar', figsize=(15, 7))
plt.title('Performance Metrics')
plt.xlabel('Metric')
plt.ylabel('Score')
plt.xticks(rotation=0)
plt.show()

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

# Plot training history
plt.figure(figsize=(12, 6))
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Training and Validation Accuracy')
plt.show()

# Test with some unseen data (using the validation data)
unseen_data = X_val[:10]  # Example: taking 10 samples from validation set
unseen_labels = y_val[:10]

# Predict using the trained model
unseen_predictions = (model.predict(unseen_data) > 0.5).astype(int)
unseen_accuracy = accuracy_score(unseen_labels, unseen_predictions)

# Display predictions on unseen data
plt.figure(figsize=(10, 6))
plt.bar(range(len(unseen_predictions)), unseen_predictions.ravel(), label='Predicted', alpha=0.6)
plt.bar(range(len(unseen_labels)), unseen_labels, label='True', alpha=0.6)
plt.xlabel('Sample Index')
plt.ylabel('Prediction')
plt.title(f'Predictions on Unseen Data (Accuracy: {unseen_accuracy:.2f})')
plt.legend()
plt.show()

import numpy as np
from tensorflow.keras.models import load_model

# Load your pre-trained model
model_path = '/content/drive/MyDrive/CI_PROJECT/pretrained_model.h5'
model = load_model(model_path)

# Create a dummy input with the expected shape
dummy_signal = np.random.rand(1, 1, 5052, 1)  # Adjust this based on your model's expected input shape

# Perform a prediction
prediction = model.predict(dummy_signal)
print("Model prediction:", prediction)

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_curve, auc
from tensorflow.keras.models import load_model
import matplotlib.pyplot as plt
import seaborn as sns

# Load the pre-trained model
model_path = '/content/drive/MyDrive/CI_PROJECT/pretrained_model.h5'
model = load_model(model_path)
print("Model loaded from", model_path)

# Load your data again or use new data for testing
# (Use the same preprocessing steps as during training)
file_path_source1 = '/content/drive/MyDrive/CI PROJECT/Datasets/Dataset/Object-1/Object-1.xlsx'
file_path_source2 = '/content/drive/MyDrive/CI PROJECT/Datasets/Dataset/Object-2/Object-2.xlsx'

data_source1 = pd.read_excel(file_path_source1, sheet_name='record_data_2020-10-29_10-41-1_', usecols="G:DZZ")
data_source2 = pd.read_excel(file_path_source2, sheet_name='record_data_2020-10-29_10-30-40', usecols="G:DZZ")

data = pd.concat([data_source1, data_source2], axis=0, ignore_index=True)
X = data.iloc[:, 7:].values  # Feature columns
y = np.concatenate([np.zeros(len(data_source1)), np.ones(len(data_source2))])  # Labels

# Check for NaNs and replace them
X = np.nan_to_num(X)

# Normalize features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Reshape features for CNN
X = X.reshape(X.shape[0], 1, X.shape[1], 1)  # Shape: (samples, height, width, channels)

# Split the data into test and validation sets if needed
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)

# Make predictions on the test set
predictions = (model.predict(X_test) > 0.5).astype(int)
probabilities = model.predict(X_test).ravel()

# Evaluate predictions
accuracy = accuracy_score(y_test, predictions)
precision = precision_score(y_test, predictions, zero_division=0)
recall = recall_score(y_test, predictions)
f1 = f1_score(y_test, predictions)
specificity = recall_score(y_test, predictions, pos_label=0)
fpr, tpr, _ = roc_curve(y_test, probabilities)
roc_auc = auc(fpr, tpr)

conf_matrix = confusion_matrix(y_test, predictions)

# Display performance metrics
metrics_df = pd.DataFrame([{
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1 Score': f1,
    'Specificity': specificity,
    'AUC': roc_auc
}])
metrics_df.plot(kind='bar', figsize=(15, 7))
plt.title('Performance Metrics')
plt.xlabel('Metric')
plt.ylabel('Score')
plt.xticks(rotation=0)
plt.show()

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_curve, auc
from tensorflow.keras.models import load_model
import matplotlib.pyplot as plt
import seaborn as sns

# Load the pre-trained model
model_path = '/content/drive/MyDrive/CI_PROJECT/pretrained_model.h5'
model = load_model(model_path)
print("Model loaded from", model_path)

# Load your data again or use new data for testing
file_path_source1 = '/content/drive/MyDrive/CI PROJECT/Datasets/Dataset/Object-1/Object-1.xlsx'
file_path_source2 = '/content/drive/MyDrive/CI PROJECT/Datasets/Dataset/Object-2/Object-2.xlsx'

data_source1 = pd.read_excel(file_path_source1, sheet_name='record_data_2020-10-29_10-41-1_', usecols="G:DZZ")
data_source2 = pd.read_excel(file_path_source2, sheet_name='record_data_2020-10-29_10-30-40', usecols="G:DZZ")

data = pd.concat([data_source1, data_source2], axis=0, ignore_index=True)
X = data.iloc[:, 7:].values  # Feature columns
y = np.concatenate([np.zeros(len(data_source1)), np.ones(len(data_source2))])  # Labels

# Check for NaNs and replace them
X = np.nan_to_num(X)

# Normalize features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Reshape features for CNN
X = X.reshape(X.shape[0], 1, X.shape[1], 1)  # Shape: (samples, height, width, channels)

# Split the data into test and validation sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)

# Make predictions on the test set
predictions = (model.predict(X_test) > 0.5).astype(int)
probabilities = model.predict(X_test).ravel()

# Map predicted values to source names
source_names = {0: 'Source #1', 1: 'Source #2'}
predicted_sources = [source_names[p[0]] for p in predictions]
actual_sources = [source_names[actual] for actual in y_test]

# Evaluate predictions
accuracy = accuracy_score(y_test, predictions)
precision = precision_score(y_test, predictions, zero_division=0)
recall = recall_score(y_test, predictions)
f1 = f1_score(y_test, predictions)
specificity = recall_score(y_test, predictions, pos_label=0)
fpr, tpr, _ = roc_curve(y_test, probabilities)
roc_auc = auc(fpr, tpr)

conf_matrix = confusion_matrix(y_test, predictions)

# Display performance metrics
metrics_df = pd.DataFrame([{
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1 Score': f1,
    'Specificity': specificity,
    'AUC': roc_auc
}])
metrics_df.plot(kind='bar', figsize=(15, 7))
plt.title('Performance Metrics')
plt.xlabel('Metric')
plt.ylabel('Score')
plt.xticks(rotation=0)
plt.show()

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

# Print predictions with source names and accuracy
print("Predicted sources and their probabilities:")
for i in range(len(predicted_sources)):
    print(f"Sample {i+1}: Predicted Source = {predicted_sources[i]}, Probability = {probabilities[i]:.4f}")

print(f"\nOverall accuracy: {accuracy:.4f}")